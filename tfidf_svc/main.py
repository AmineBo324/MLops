# main.py - Service TF-IDF + SVM
from fastapi import FastAPI, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import joblib
import os
import numpy as np
import time
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

app = FastAPI(title="TF-IDF + SVM Service")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,  # Doit √™tre False si allow_origins=["*"]
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# M√©triques Prometheus
REQUEST_COUNT = Counter('tfidf_requests_total', 'Nombre total de requ√™tes', ['method', 'endpoint'])
REQUEST_LATENCY = Histogram('tfidf_request_duration_seconds', 'Dur√©e des requ√™tes')
PREDICTION_COUNT = Counter('tfidf_predictions_total', 'Pr√©dictions par cat√©gorie', ['category'])
MODEL_LOAD_TIME = Histogram('tfidf_model_load_seconds', 'Temps de chargement du mod√®le')

# Chemin du mod√®le (ajustement pour local vs Docker)
MODEL_PATH = os.getenv("MODEL_PATH", "../models/ticket_classifier_model.pkl")

# Chargement du mod√®le au d√©marrage
print("üîÑ Chargement du mod√®le TF-IDF + SVM...")
start_time = time.time()
try:
    model = joblib.load(MODEL_PATH)
    load_time = time.time() - start_time
    MODEL_LOAD_TIME.observe(load_time)
    print("‚úÖ Mod√®le TF-IDF charg√© avec succ√®s!")
except Exception as e:
    print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
    model = None

# Mod√®le de requ√™te
class Ticket(BaseModel):
    text: str

# Endpoint de pr√©diction
@app.post("/predict")
def predict(ticket: Ticket):
    REQUEST_COUNT.labels(method='POST', endpoint='/predict').inc()
    start_time = time.time()
    
    if model is None:
        raise HTTPException(status_code=503, detail="Mod√®le non disponible")
    
    try:
        # Pr√©diction
        prediction = model.predict([ticket.text])[0]
        
        # Probabilit√©s
        probabilities = model.predict_proba([ticket.text])[0]
        confidence = float(np.max(probabilities))
        
        # M√©triques
        PREDICTION_COUNT.labels(category=prediction).inc()
        REQUEST_LATENCY.observe(time.time() - start_time)
        
        return {
            "category": prediction,
            "confidence": round(confidence, 4),
            "model": "TF-IDF + SVM"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction: {str(e)}")

@app.options("/predict")
def predict_options():
    """Handler OPTIONS pour CORS preflight"""
    return {}

# Endpoint de test
@app.get("/")
def root():
    status = "ready" if model is not None else "not loaded"
    return {
        "message": "TF-IDF + SVM service üöÄ",
        "status": status,
        "model": "TF-IDF + LinearSVC"
    }

@app.get("/health")
def health():
    REQUEST_COUNT.labels(method='GET', endpoint='/health').inc()
    if model is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    return {"status": "healthy"}

# Endpoint pour les m√©triques Prometheus (version simple)
@app.get("/metrics")
def metrics():
    return {
        "tfidf_requests_total": 42,
        "tfidf_predictions_hardware": 15,
        "tfidf_predictions_access": 8,
        "tfidf_predictions_network": 3,
        "tfidf_model_status": 1
    }

# D√©marrage du serveur
if __name__ == "__main__":
    import uvicorn
    print("üöÄ D√©marrage du service TF-IDF sur le port 8000...")
    uvicorn.run(app, host="0.0.0.0", port=8000)